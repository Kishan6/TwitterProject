# -*- coding: utf-8 -*-
"""TweetClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wfv3oViTtpy29ETV6fLP7O0nxhDG8XFC

#Read Input
"""

import pandas as pd  
import numpy as np
import matplotlib.pyplot as plt

#Upload file
from google.colab import files
uploaded = files.upload()

import io
df2 = pd.read_csv(io.BytesIO(uploaded['twitter_data_6.csv']))
# Dataset is now stored in a Pandas Dataframe

df2.head()

df3 = df2.loc[:, ["tweet","class"]]

import matplotlib.pyplot as plt
fig = plt.figure(figsize=(8,6))
df3.groupby('class').tweet.count().plot.bar(ylim=0)
plt.show()

"""#Clean Data"""

# from https://towardsdatascience.com/cleaning-text-data-with-python-b69b47b97b76
import re

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import re
import nltk
import string
from nltk.corpus import stopwords
# # In case of any corpus are missing 
# download all-nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger') 
stop_words = stopwords.words("english")
from nltk.stem import WordNetLemmatizer
wordnet = WordNetLemmatizer()
def text_preproc(x):
  x = x.lower()
  x = ' '.join([word for word in x.split(' ') if word not in stop_words])
  x = x.encode('ascii', 'ignore').decode()
  x = re.sub(r'https*\S+', ' ', x)
  x = re.sub(r'@\S+', ' ', x)
  x = re.sub(r'#\S+', ' ', x)
  x = re.sub(r'\'\w+', '', x)
  x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)
  x = re.sub(r'\w*\d+\w*', '', x)
  x = re.sub(r'\s{2,}', ' ', x)
  return x

def numberize(x):
  if (x == 'depressed'):
    return (0)
  else:
    return (1)

# clean data
print(df3.to_numpy()[5][0])
print(text_preproc(df3.to_numpy()[5][0]))
df3["tweet"] = df3["tweet"].apply(lambda x: text_preproc(x))
df3["class"] = df3["class"].apply(lambda x: numberize(x))

"""#Naive Bayes classifier(Using one hot encoding)"""

#Training test split
from sklearn.model_selection import train_test_split

train, test = train_test_split(df3, test_size=0.5)

#build bag of words
import nltk 
from nltk.tokenize import word_tokenize

def bag_words(tweet):
  return [word for word in tweet.split()]

processed_train = []
processed_test = []
for index in range(len(train['tweet'])):
  processed_train.append((
      bag_words(train.iloc[index][0]), train.iloc[index][1]))
for index in range(len(test['tweet'])):
  processed_test.append((
      bag_words(test.iloc[index][0]), test.iloc[index][1]))
# processed_train['text'] = processed_train['text'].apply(lambda x: bag_words(x))
# processed_test['text'] = processed_test['text'].apply(lambda x: bag_words(x))
print(processed_test[0])

def buildVocabulary(preprocessedTrainingData):
    all_words = []
    
    for (words, sentiment) in preprocessedTrainingData:
        all_words.extend(words)

    wordlist = nltk.FreqDist(all_words)
    word_features = []

    for key in wordlist.keys():

      # change this to fix size of dictionery
      if wordlist[key] > 50:
        word_features.append(key)
    
    return word_features

def extract_features(tweet):
    tweet_words = set(tweet)
    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in tweet_words)
    return features

word_features = buildVocabulary(processed_train)
print(len(word_features))
trainingFeatures = nltk.classify.apply_features(extract_features, processed_train)

NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)

NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in processed_test]

pos = 0
neg = 0
for index in range(len(processed_test)):
  if (processed_test[index][1] == NBResultLabels[index]):
    pos += 1
  else:
    neg += 1
print("Accuracy = ", 1.0 * pos / (pos+neg), "%")

"""#Naive Bayer Classifier(Using Tf-idf encoding)"""

import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer

description_list = df3['tweet'].tolist()

count_vect = CountVectorizer(max_features=10000)
x_train_counts = count_vect.fit_transform(description_list)


tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)

train_x, test_x, train_y, test_y = train_test_split(x_train_tfidf, df2['class'].tolist(), test_size=0.5)
print(train_x[0])

clf = MultinomialNB().fit(train_x, train_y)
y_score = clf.predict(test_x)

n_right = 0
for i in range(len(y_score)):
    if y_score[i] == test_y[i]:
        n_right += 1

print("Accuracy: %.2f%%" % ((n_right/float(len(test_y)) * 100)))

"""#Logistic regression"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer

description_list = df3['tweet'].tolist()

count_vect = CountVectorizer(max_features=10000)
x_train_counts = count_vect.fit_transform(description_list)


tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)

train_x, test_x, train_y, test_y = train_test_split(x_train_tfidf, df2['class'].tolist(), test_size=0.5)
print(train_x[0])

clf = LogisticRegression(max_iter = 500).fit(train_x, train_y)
y_score = clf.predict(test_x)

n_right = 0
for i in range(len(y_score)):
    if y_score[i] == test_y[i]:
        n_right += 1

print("Accuracy: %.2f%%" % ((n_right/float(len(test_y)) * 100)))

"""#SVM Classifier(Using Tf-Idf encoding)"""

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.svm import SVC

description_list = df3['tweet'].tolist()

count_vect = CountVectorizer(max_features=10000)
x_train_counts = count_vect.fit_transform(description_list)


tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)

train_x, test_x, train_y, test_y = train_test_split(x_train_tfidf, df2['class'].tolist(), test_size=0.7)

clf = SVC(kernel='linear').fit(train_x, train_y)
y_score = clf.predict(test_x)

n_right = 0
for i in range(len(y_score)):
    if y_score[i] == test_y[i]:
        n_right += 1

print("Accuracy: %.2f%%" % ((n_right/float(len(test_y)) * 100)))

"""#Random forest Classifier(Using Tf-Idf encoding)"""

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.ensemble import RandomForestClassifier

description_list = df3['tweet'].tolist()

count_vect = CountVectorizer(max_features=10000)
x_train_counts = count_vect.fit_transform(description_list)


tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)

train_x, test_x, train_y, test_y = train_test_split(x_train_tfidf, df2['class'].tolist(), test_size=0.5)

clf = RandomForestClassifier(n_estimators = 100, max_depth=20)   
clf.fit(train_x, train_y)

# performing predictions on the test dataset 
y_pred = clf.predict(test_x) 
  # metrics are used to find accuracy or error 
from sklearn import metrics   
print() 
# using metrics module for accuracy calculation 
print("ACCURACY OF THE MODEL: ", metrics.accuracy_score(test_y, y_pred))

# performing predictions on the test dataset 
y_pred = clf.predict(train_x) 
  # metrics are used to find accuracy or error 
from sklearn import metrics   
print() 
# using metrics module for accuracy calculation 
print("ACCURACY OF THE MODEL: ", metrics.accuracy_score(train_y, y_pred))
print(train_x[0], train_y[0])